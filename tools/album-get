#!/bin/bash
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
FULLALBUM=1
. $DIR/lib.sh

declare -i TTL=0
declare -A site_regex
site_regex[bandcamp]='(?<=")[\w:\/\.]+(?<=(album|track)\/)[^?'"'"'"]*' 
site_regex[archive]='(?<=")/details/[^"@&]+(?=..title)'

if [[ $# = 0 ]]; then
  export COUNTFILE=$(mktemp)

  for i in $(ls); do
    if [[ -d $i ]]; then
      ${BASH_SOURCE[0]} $i
      check_for_stop
      TTL+=$(< $COUNTFILE )
    fi
  done

  echo "(Total) Added $TTL releases $(date)"
  exit
fi

base_path=$PWD
while [[ $# -gt 0 ]]; do
  check_for_stop
  arg=${1%%.}

  # We start with a bandcamp assumption
  regex=${site_regex[bandcamp]}
  FORMAT=mp3-128
  if [[ $arg =~ \. ]]; then
    if [[ $arg =~ archive.org ]]; then
      base=$(basename "$arg")
      parser="archive"
      # Do an override here.
      regex=${site_regex[archive]}
      base_url=$arg
      FORMAT=0
    elif [[ -n "$2" ]]; then 
      base="$2" 
      domain=$arg
      base_url=$arg
      shift
    else 
      base=$( echo $arg | sed -n 's/\..*$//p' )
      base_url=$arg
    fi
    extra=
  else
    base=$arg
    base_url=$base.bandcamp.com
    extra=music
  fi
  shift

  [[ -e $base ]] || mkdir $base

  base_name=$base
  if [[ -e $base/domain ]]; then
    base_url=$(< $base/domain )
    base_name=$base_url
  elif [[ -n "$domain" ]]; then
    echo $base_url > $base/domain
  fi
  ttl=$(ls $base | grep -v domain | wc -l)

  starting_point="$PWD/$base"

  if [[ -f "$starting_point/no" ]]; then
    echo "! $base_name purged"
    exit 0
  else
    echo "♫ $base_name ♫ ($ttl)"
  fi

  url=$base_url/$extra
  [[ $url =~ http ]] || url=https://$url

  for full in $(curl -sL $url | grep -Po $regex | sort | uniq ); do
    check_for_stop
    # sometimes we get x-dom references, sometimes it's relevant. We resolve that below.
    release=$(basename "$full")
    place="$starting_point/$release"
    isnew=
    manual_pull=

    if [[ ! -e "$place" ]]; then
      isnew=true
      mkdir "$place"
    fi

    # if we haven't any files then we just try to download from it again... 
    exitcode=0
    [[ -e $place/exit-code ]] && exitcode=$(< $place/exit-code )

    # Make sure the path is full
    count=$( /bin/ls "$place" 2> /dev/null | /bin/grep -Ev '^(page.html|exit-code|album-art.jpg|domain)$' | /usr/bin/wc -l )

    # [[ "$count" = "1" ]] && [[ ! -e "$place"/no ]] && ls "$place/"
    if [[ -e "$place/no" ]]; then 
      printf "  -  "
    else
      printf "  %-2d " $count 
    fi
    echo $release

    force_try=
    if [[ $count -lt 4 && -e "$place/$PAGE" ]]; then
      release_date=$(grep -m 1 -Po '((?<=releases )[A-Z][a-z]+ [0-9]{1,2}, 20[0-9]{2})' "$place/$PAGE" )

      # If our record of the page contains "releases" instead of "released" 
      # it's worth trying to pull this down again since the time might have
      # now lapsed.
      if [[ -n "$release_date" ]]; then
        release_date_unix=$(date --date="$release_date" +%s)
        from_now=$(( release_date_unix - $(date +%s) ))
        if [[ $from_now -lt 0 ]]; then
          echo -e "\t$release_date is in the past"

          # Second confirmation.
          page_download_date=$(stat -c %Z  "$place/$PAGE")

          ## we'll add a couple day buffer to the relase date for timezones and delays
          if [[ $page_download_date -gt $(( release_date_unix + 172800 )) ]]; then
            echo -e "\tHOWEVER, $PAGE is newer than that date"
          else
            force_try=0
            _rm "$place/no-files"
          fi
        else
          echo -e "\tFUTURE: $release_date ... skipping"
        fi
      fi
    fi

    if [[ "$count" = "0" || $exitcode != "0" || -n "$force_try" ]]; then
      _rm "$place/exit-code"
      echo "--- $place"

      # here's where we look for the full url
      if [[ $parser == 'archive' ]]; then
        url=https://archive.org$full
      elif [[ $full =~ : ]]; then
        url=$full
      else
        url=https://$base_url$full
      fi
      echo "  ⇩ $url"
      echo $url > "$place"/domain

      [[ -e "$place"/no-files ]] && continue

      # this means we've been here before. We can use the -g 
      # option to see if nothing ought to be downloaded in which
      # case we mark it as skippable
      if [[ -z "$isnew" ]]; then
        # this is a manual scrape for youtube-dl which can get confused.
        manual_pull=0

        # we put in some kind of backoff strategy
        sleep 3
        track_count=$( youtube-dl -if mp3-128 -g -- "$url" | wc -l )
        if [[ "$track_count" = "0" ]]; then
          echo "  ( no files )"
          touch "$place"/no-files
          continue
        fi
      fi

      # sometimes people are posting wav files ... really, that's crazy
      if [[ -n "$FULLALBUM" ]]; then 
        if [[ -n "$manual_pull" ]]; then
          manual_pull "$url" "$place"

        else
          get_mp3s "$url" "$place"
        fi
      else
        get_urls "$url" "$place"
      fi
      get_page "$place"

      echo "$(date +%Y%m%d) $place $url" >> "$base_path/.dl_history"
      (( TTL ++ ))
    fi
  done
done

[[ -n "$COUNTFILE" ]] && echo $TTL > $COUNTFILE
[[ $TTL -gt 1 ]] && echo "Added $TTL"
