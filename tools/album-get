#!/bin/bash
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
FULLALBUM=1
final_judgement=

# Setting this will allow you to bypass the curl for releases
# by using the cached version for the label. 
#USECACHE=
. $DIR/lib.sh

declare -i TTL=0
declare -A site_regex

[[ -n "$DEBUG" ]] && set -x

site_regex[bandcamp]='(?<=("|;))[\-\w:\/\.]+(?<=(album|track)\/)[^\&?'"'"'"]*' 
site_regex[archive]='(?<=")/details/[^"@&]+(?=..title)'

if [[ $# = 0 ]]; then
  export COUNTFILE=$(mktemp)

  for i in $(ls); do
    if [[ -d "$i" ]]; then
      # it DOES NOT loop, it reruns it every time
      #
      # If we just cycle through the labels with no update you'll
      # eventually get a 429 so we need to use our backoff strategy
      #
      # The one exception is if we use the cache
      if [[ -z "$USECACHE" || ! -e "$i/$PAGE" ]]; then
        now=$(date +%s)
        if (( now - lastrun < SLEEP_MIN )); then
          ttl=$(( SLEEP_MIN - (now - lastrun) ))
          echo "  !! Moving too quickly, sleeping ${ttl}s"
          sleep $ttl
        fi
        lastrun=$now
      fi

      ${BASH_SOURCE[0]} "$i"
      check_for_stop
      TTL+=$(< $COUNTFILE )
    fi
  done

  echo "(Total) Added $TTL releases $(date)"
  exit
fi

base_path=$PWD
while [[ $# -gt 0 ]]; do
  check_for_stop
  arg=${1%%.}

  list=
  # Always attempt to look to see if new things/updates?
  always_attempt=
  # We start with a bandcamp assumption
  regex=${site_regex[bandcamp]}
  FORMAT="-f mp3-128"
  #
  # base_name   the hostname
  # base        the path
  # base_url    ??
  # domain      ??
  #
  # (wrote this 5 years ago, can't figure it out.)
  #
  if [[ "$arg" =~ \. ]]; then
    if [[ "$arg" =~ archive.org ]]; then
      base=$(basename "$arg")
      parser="archive"
      # Do an override here.
      regex=${site_regex[archive]}
      base_url="$arg"
      FORMAT=
    elif [[ "$arg" =~ soundcloud.com ]]; then
      # there is no descent, we use the base
      list=($arg)
      base=$(basename "$arg")
      domain="$arg"
      always_attempt=1
      base_url="$arg"
      FORMAT=
    # This means we are getting a single album
    elif [[ "$arg" =~ bandcamp.com/album/ ]]; then
      eval $(echo "$arg" | awk -F\/ ' { sub(/\..*/,"",$3);print "base="$3";list=/"$(NF-1)"/"$NF }')
      base_url="$base.bandcamp.com"
    elif [[ -n "$2" ]]; then 
      base="$2" 
      domain="$arg"
      base_url="$arg"
      shift
    else 
      base=$( echo "$arg" | sed -n 's/\..*$//p' )
      base_url="$arg"
    fi
    extra=
  # We are looking for a path with a domain file already there
  # This doesn't really work but here's a workaround
  #
  # mpv-lib single_album some_url
  #
  # And it dumps it in the current directory
  #
  elif [[ "$arg" =~ '/' && -d "$base_path/$arg" ]]; then 
    echo hi
    base="$arg"
    list=$(< "$arg/domain" )
  else
    base="$arg"
    base_url="$base.bandcamp.com"
    extra=music
  fi
  shift

  _mkdir "$base"

  base_name="$base"
  if [[ -s "$base/domain" ]]; then
    base_url=$(< "$base/domain" )
    base_name="$base_url"
  elif [[ -n "$domain" ]]; then
    echo "$base_url" > "$base/domain"
  fi
  ttl=$(ls "$base" | grep -v domain | wc -l)

  starting_point="$PWD/$base"

  if [[ -f "$starting_point/no" ]]; then
    echo "! $base_name purged"
    continue
  else
    echo "♫ $base_name ♫ ($ttl)"
  fi

  url="$base_url/$extra"
  [[ "$url" =~ http ]] || url="https://$url"

  if [[ -z "$list" ]]; then
    [[ -n "$USECACHE" && -s "$base/$PAGE" ]] || curl -sL "$url" > "$base/$PAGE"
    list=$(grep -Po "$regex" "$base/$PAGE" | sort | uniq)
  fi

  #echo $url $list; exit
  # If we didn't get anything then the domain is "probably" gone, let's record that.
  if [[ -z "$list" ]]; then
    echo "  !! ~~~ $base @ $url Disappeared! ~~~" 
    echo "$url $base $(date +%Y%m%d)" >> "$base_path/.dl_failure"
  fi

  for full in $list; do
    release=$(basename "$full")
    place="$starting_point/$release"

    if [[ -e "$place/no" ]]; then 
      #echo   "     $release"
      continue
      # Pay attention to the -s, essentially we 
      # make a final judgement below if there's really
      # no files.
    elif [[ -s "$place/no-files" ]]; then
      echo   "  0  $release"
      continue
    fi

    check_for_stop
    # sometimes we get x-dom references, sometimes it's relevant. We resolve that below.
    isnew=
    manual_pull_flag=

    if [[ ! -e "$place" ]]; then
      isnew=true
      mkdir "$place"
    fi

    # if we haven't any files then we just try to download from it again... 
    exitcode=0
    [[ -e "$place/exit-code" ]] && exitcode=$(< $place/exit-code )

    # Make sure the path is full
    count=$( /bin/ls "$place" 2> /dev/null | /bin/grep -Ev '^(page.html|exit-code|album-art.jpg|domain)$' | /usr/bin/wc -l )

    # [[ "$count" = "1" ]] && [[ ! -e "$place"/no ]] && ls "$place/"
    printf "  %-2d %s\n" $count $release

    force_try=
    if [[ -n "$always_attempt" || ( $count -lt 4 && -e "$place/$PAGE" ) ]]; then
      release_date=$(grep -m 1 -Po '((?<=releases )[A-Z][a-z]+ [0-9]{1,2}, 20[0-9]{2})' "$place/$PAGE" )

      # If our record of the page contains "releases" instead of "released" 
      # it's worth trying to pull this down again since the time might have
      # now lapsed.
      if [[ -n "$release_date" ]]; then
        release_date_unix=$(date --date="$release_date" +%s)
        from_now=$(( release_date_unix - $(date +%s) ))
        if [[ $from_now -lt 0 ]]; then
          echo -e "\t$release_date is in the past"

          # Second confirmation.
          # We are trying to get the minvalue of the mtime and ctime. 
          # mtime can be < ctime, who knew?
          page_download_date=$(xargs -n1 <<< "$(stat -c %Z "$place/$PAGE") $(stat -c %Y "$place/$PAGE")" | sort -n | head -n 1)

          ## we'll add a couple weeks buffer because sometimes people release things
          ## gradually overtime This is seconds in day * days
          buffer_days=21
          buffer=$(( 86400 * buffer_days ))
          if [[ $page_download_date -gt $(( release_date_unix + buffer )) ]]; then
            echo -e "\tHOWEVER, $PAGE is newer than that date + $buffer_days day(s)"
          else
            force_try=0
            final_judgement=0
            ## This isn't smart because it forces a check every time.
            # Sometimes there's honestly, truly no files.
            _rm "$place/no-files"
          fi
        else
          echo -e "\tFUTURE: $release_date ... skipping"
        fi
      fi
    fi

    if [[ "$count" = "0" || $exitcode != "0" || -n "$force_try" ]]; then
      _rm "$place/exit-code"
      echo "--- $place"

      # here's where we look for the full url
      if [[ $parser == 'archive' ]]; then
        url=https://archive.org$full
      elif [[ $full =~ : ]]; then
        url=$full
      else
        url=https://$base_url$full
      fi
      echo "  ⇩ $url"
      echo "$url" > "$place"/domain

      [[ -e "$place"/no-files ]] && continue

      # this means we've been here before. We can use the -g 
      # option to see if nothing ought to be downloaded in which
      # case we mark it as skippable
      if [[ -z "$isnew" ]]; then
        # this is a manual scrape for $YTDL which can get confused.
        manual_pull_flag=0

        # we put in some kind of backoff strategy
        sleep $SLEEP_MIN
        track_count=$( $YTDL -i $FORMAT -g -- "$url" | wc -l )
        if [[ "$track_count" = "0" ]]; then
          echo "  ( no files )"
          if [[ -n "$final_judgement" ]] ; then
            final_judgement=
            # We give the no-files a non-zero size to tell 
            # us to REALLY go away
            echo "." > "$place"/no-files
          fi
          touch "$place"/no-files
          continue
        fi
      fi

      # sometimes people are posting wav files ... really, that's crazy
      if [[ -n "$FULLALBUM" ]]; then 
        [[ -n "$manual_pull_flag" ]] && manual_pull "$url" "$place" || get_mp3s "$url" "$place"
      else
        get_urls "$url" "$place"
      fi
      get_page "$place"

      echo "$(date +%Y%m%d) $place $url" >> "$base_path/.dl_history"
      (( TTL ++ ))
    fi
  done
done

[[ -n "$COUNTFILE" ]] && echo $TTL > $COUNTFILE
[[ $TTL -gt 1 ]] && echo "Added $TTL"
